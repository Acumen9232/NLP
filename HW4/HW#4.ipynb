{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf40d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.961 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: word2vec -train corpusSegDone.txt -output corpusWord2Vec.txt -size 300 -window 5 -sample 1e-3 -hs 0 -negative 5 -threads 12 -iter 5 -min-count 5 -alpha 0.025 -debug 2 -binary 0 -cbow 1\n",
      "Starting training using file corpusSegDone.txt\n",
      "Vocab size: 4527\n",
      "Words in train file: 175748\n",
      "Alpha: 0.008303  Progress: 72.85%  Words/thread/sec: 1185.48k  "
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import word2vec\n",
    "import json\n",
    "import numpy as np\n",
    "from os import walk\n",
    "from os.path import join\n",
    "from opencc import OpenCC\n",
    "\n",
    "fileTrainRead = []\n",
    "fileTrainSeg=[]\n",
    "path =\"wiki_zh\"\n",
    "for root, dirs, files in walk(path):\n",
    "    for f in files:\n",
    "        fullpath = join(root, f)\n",
    "        input_file = open(fullpath, encoding = 'utf8')\n",
    "        for line in input_file.readlines():\n",
    "            fileTrainRead.append(json.loads(line))\n",
    "        input_file.close()\n",
    "\n",
    "cc = OpenCC('s2t')\n",
    "for word in fileTrainRead:\n",
    "    word['title'] = cc.convert(word['title'])\n",
    "    word['text'] = cc.convert(word['text'])\n",
    "\n",
    "for i in fileTrainRead:\n",
    "    fileTrainSeg.append([' '.join(list(jieba.cut(i['text'],cut_all=False)))])   \n",
    "\n",
    "fileSegWordDonePath ='corpusSegDone.txt'\n",
    "with open(fileSegWordDonePath,'wb') as fW:\n",
    "    for i in range(len(fileTrainSeg)):\n",
    "        fW.write(fileTrainSeg[i][0].encode('utf-8'))\n",
    "        fW.write('\\n'.encode('utf-8'))\n",
    "word2vec.word2vec('corpusSegDone.txt', 'corpusWord2Vec.txt', size = 300, verbose = True)\n",
    "model = word2vec.load('corpusWord2Vec.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bade1b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   記錄\n",
      "2   階級\n",
      "3   知識\n",
      "4   數學\n",
      "5   生物\n",
      "6   一系列\n",
      "7   主義\n",
      "8   牀\n",
      "9   教育\n",
      "10   道德\n",
      "11   解釋\n",
      "12   巨大\n",
      "13   應\n",
      "14   古希臘\n",
      "15   哲學\n",
      "16   文化\n",
      "17   涉及\n",
      "18   活動\n",
      "19   結合\n",
      "20   單位\n"
     ]
    }
   ],
   "source": [
    "word = '人'\n",
    "n = 20\n",
    "\n",
    "metrics = np.dot(model.vectors, model[word].T)\n",
    "best = np.argsort(metrics)[::-1][1:n+1]\n",
    "for i, index in enumerate(best):\n",
    "    print(i+1, ' ', model.vocab[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
